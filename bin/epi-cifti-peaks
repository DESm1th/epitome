#!/usr/bin/env python
"""
Takes a cifti map ('dscalar.nii' and outputs a csv of peak vertices)

Usage:
    epi-cifti-peaks [options] <func.dscalar.nii> <L.surf.gii> <R.surf.gii>

Arguments:
    <func.dscalar.nii>    Input map.
    <L.surf.gii>          Corresponding Left surface
    <R.surf.gii>          Corresponding Right surface file

Options:
    --surface-distance MM  minimum distance in mm [default: 40] between extrema of the same type.
    --volume-distance MM   minimum distance in mm [default: 40] between extrema of the same type.
    --min-threshold MIN    the largest value [default: -2.85] to consider for being a minimum
    --max-threshold MAX    the smallest value [default: 2.85] to consider for being a maximum
    --outputcsv FILE       Filename of the output csv table
    --debug                Debug logging
    -n,--dry-run           Dry run
    -h, --help             Prints this message

DETAILS
Note: at the moment this only outputs the peaks from the surface component of the cifti map.
Use -cifti-separate in combination with FSL's clusterize to get the volume peaks

Default name for the output csv taken from the input file.
i.e. func.dscalar.nii --> func_peaks.csv

Written by Erin W Dickie, June 3, 2016
"""

import os
import sys
import subprocess
import numpy as np
import scipy as sp
import nibabel as nib
import pandas as pd
import tempfile
import shutil
import nibabel.gifti.giftiio
import epitome as epi
from epitome.docopt import docopt

## make the tempdir
tmpdir = tempfile.mkdtemp()

def docmd(cmdlist):
    "sends a command (inputed as a list) to the shell"
    if DEBUG: print ' '.join(cmdlist)
    if not DRYRUN: subprocess.call(cmdlist)

def load_surfaceonly(filename, tempdir = tmpdir):
    '''
    separate a cifti file into surfaces,
    then loads and concatenates the surface data
    '''
    ## separate the cifti file into left and right surfaces
    L_data_surf=os.path.join(tempdir, 'Ldata.func.gii')
    R_data_surf=os.path.join(tempdir, 'Rdata.func.gii')
    docmd(['wb_command','-cifti-separate', filename, 'COLUMN',
        '-metric', 'CORTEX_LEFT', L_data_surf,
        '-metric', 'CORTEX_RIGHT', R_data_surf])

    ## load both surfaces and concatenate them together
    Ldata = epi.utilities.load_gii_data(L_data_surf)
    Rdata = epi.utilities.load_gii_data(R_data_surf)

    return Ldata, Rdata

def main():
    global DEBUG
    global DRYRUN
    global tmpdir

    arguments = docopt(__doc__)
    data_file = arguments['<func.dscalar.nii>']
    surfL = arguments['<L.surf.gii>']
    surfR = arguments['<R.surf.gii>']
    surf_distance = arguments['--surface-distance']
    volume_distance = arguments['--volume-distance']
    min_threshold = arguments['--min-threshold']
    max_threshold = arguments['--max-threshold']
    outputcsv = arguments['--outputcsv']
    DEBUG = arguments['--debug']
    DRYRUN = arguments['--dry-run']

    ## run wb_command -cifti-extrema to find the peak locations
    extrema_file = os.path.join(tmpdir,'extrema.dscalar.nii')
    docmd(['wb_command',
           '-cifti-extrema',
           data_file,
           str(surf_distance), str(volume_distance),'COLUMN',
           extrema_file,
           '-left-surface', surfL, '-right-surface', surfR,
           '-threshold', str(min_threshold), str(max_threshold)])

    ## read in the extrema file from above
    Lextrema, Rextrema = load_surfaceonly(extrema_file, tempdir = tmpdir)
    Lverts = np.nonzero(Lextrema)[0]  # indices - vertex id for peak in left
    Rverts = np.nonzero(Rextrema)[0]  # indices - vertex id for peak in right

    ## read in the original data for the value column
    Ldata, Rdata = load_surfaceonly(data_file, tempdir = tmpdir)

    ## read in the surface mni coordinates
    surf_dist_nib = nibabel.gifti.giftiio.read(surfL)
    Lcoords = surf_dist_nib.getArraysFromIntent('NIFTI_INTENT_POINTSET')[0].data
    surf_dist_nib = nibabel.gifti.giftiio.read(surfR)
    Rcoords = surf_dist_nib.getArraysFromIntent('NIFTI_INTENT_POINTSET')[0].data

    ## put all this info together into one pandas dataframe
    dfL = pd.DataFrame({"hemisphere": 'L',
                    "vertex": Lverts,
                    'x': np.round(Lcoords[Lverts,0]),
                    'y': np.round(Lcoords[Lverts,1]),
                    'z': np.round(Lcoords[Lverts,2]),
                    'value': np.reshape(Ldata[Lverts],(len(Lverts),))})
    dfR = pd.DataFrame({"hemisphere": 'R',
                        "vertex": Rverts,
                        'x': np.round(Rcoords[Rverts,0]),
                        'y': np.round(Rcoords[Rverts,1]),
                        'z': np.round(Rcoords[Rverts,2]),
                        'value': np.reshape(Rdata[Rverts],(len(Rverts),))})
    df = dfL.append(dfR, ignore_index = True)

    ## if not outputname is given, create it from the input dscalar map
    if not outputcsv:
        outputcsv = os.path.basename(data_file).replace('.dscalar.nii','_peaks.csv')

    ## write the table out to the outputcsv
    df.to_csv(outputcsv,
          columns = ['hemisphere','vertex','x','y','z', 'value'],
          index=False)
    ## remove the tempdirectory
    shutil.rmtree(tmpdir)

if __name__ == '__main__':
    main()
